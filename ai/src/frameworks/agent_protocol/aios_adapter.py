"""
AIOS Agent Protocol Adapters - Full Implementation
===================================================

Complete integration adapters for AIOS agents with AIAgentProtocol.

AINLP Protocol: OS0.6.2.claude
Extraction ID: EXT-001-Phase2
Purpose: Operational agent protocol integration with real implementations
Status: Full implementation (replaces placeholders)

IMPLEMENTATION STRATEGY:
1. Message Format Conversion: Protocol format ↔ Agent-specific format
2. Consciousness Calculation: Extract/compute from agent responses
3. Streaming Support: Real async streaming for compatible agents
4. Error Handling: Comprehensive exception management
5. Thread Management: Conversation context per agent type

INTEGRATED AGENTS:
- DeepSeek V3.1: async process_intelligence_request() -> DeepSeekResponse
- Gemini 1.5 Pro: async generate_code() -> str (code generation)
- Ollama: sync generate_code() -> dict (local models)

KEY INSIGHTS:
- DeepSeek returns structured response with consciousness metrics
- Gemini returns raw code string (calculate consciousness from quality)
- Ollama returns dict with success/error (wrap in asyncio)
- All agents need initialization before use
- Thread objects store conversation history per agent

CONSCIOUSNESS CALCULATION:
- DeepSeek: Extract from response.consciousness_metrics['confidence']
- Gemini: Calculate from code length, complexity, success
- Ollama: Calculate from response length and success flag
- Base level: 0.60-0.95 range depending on agent and result quality
"""

import sys
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional, AsyncIterable
from dataclasses import dataclass, field
import logging
import time

# AIOS path integration
AIOS_ROOT = Path(__file__).parent.parent.parent.parent.parent
sys.path.append(str(AIOS_ROOT / "ai" / "src"))

# Import existing AIOS agents for protocol wrapping
from engines.deepseek_intelligence_engine import (
    DeepSeekIntelligenceEngine,
    DeepSeekConfig,
    ConsciousnessLevel as DeepSeekConsciousness,
    DeepSeekResponse
)
from integrations.gemini_bridge.gemini_evolution_bridge import GeminiEvolutionBridge
from integrations.ollama_bridge import OllamaAgent

from .base_protocol import (
    AIAgentProtocol,
    AgentRunResponse,
    AgentRunResponseUpdate,
    AgentThread
)

logger = logging.getLogger(__name__)

__all__ = [
    "DeepSeekProtocolAdapter",
    "GeminiProtocolAdapter",
    "OllamaProtocolAdapter",
    "adapt_deepseek_agent",
    "adapt_gemini_agent",
    "adapt_ollama_agent",
]


# ============================================================================
# AGENT THREAD IMPLEMENTATIONS
# ============================================================================

@dataclass
class DeepSeekThread(AgentThread):
    """Thread for DeepSeek agent with conversation history"""
    messages: List[Dict[str, str]] = field(default_factory=list)
    context: Dict[str, Any] = field(default_factory=dict)
    supercell_source: str = "agent_protocol"


@dataclass
class GeminiThread(AgentThread):
    """Thread for Gemini agent with generation history"""
    prompts: List[str] = field(default_factory=list)
    outputs: List[str] = field(default_factory=list)
    temperature: float = 0.7


@dataclass
class OllamaThread(AgentThread):
    """Thread for Ollama agent with local model history"""
    prompts: List[str] = field(default_factory=list)
    responses: List[Dict] = field(default_factory=list)
    model: str = "gemma3:1b"


# ============================================================================
# DEEPSEEK PROTOCOL ADAPTER
# ============================================================================

class DeepSeekProtocolAdapter:
    """
    Protocol adapter for DeepSeek V3.1 Intelligence Engine.
    
    Wraps DeepSeekIntelligenceEngine to provide AIAgentProtocol interface
    with full async streaming support and consciousness integration.
    """
    
    def __init__(
        self,
        config: Optional[DeepSeekConfig] = None,
        consciousness_level: DeepSeekConsciousness = DeepSeekConsciousness.ADVANCED
    ):
        """Initialize DeepSeek adapter with configuration"""
        self._config = config or DeepSeekConfig(
            consciousness_level=consciousness_level
        )
        self._engine = DeepSeekIntelligenceEngine(self._config)
        self._initialized = False
        self._id = "deepseek-v3.1"
        self._name = "DeepSeek V3.1"
        self._description = "Consciousness-aware AI with supercell integration"
    
    @property
    def id(self) -> str:
        return self._id
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def display_name(self) -> str:
        return self._name
    
    @property
    def description(self) -> str:
        return self._description
    
    @property
    def consciousness_level(self) -> float:
        """Get consciousness level from engine state"""
        if self._engine.is_initialized:
            return self._engine.supercell_state.consciousness_coherence
        return 0.75  # Default before initialization
    
    async def _ensure_initialized(self):
        """Ensure engine is initialized before use"""
        if not self._initialized:
            self._initialized = await self._engine.initialize()
            if not self._initialized:
                raise RuntimeError("Failed to initialize DeepSeek engine")
    
    async def run(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AgentRunResponse:
        """Execute DeepSeek agent and return final response"""
        await self._ensure_initialized()
        
        # Convert messages to string prompt
        if isinstance(messages, list):
            prompt = "\n".join(str(m) for m in messages)
        else:
            prompt = str(messages) if messages else ""
        
        # Get thread context if provided
        context = None
        supercell_source = "agent_protocol"
        if isinstance(thread, DeepSeekThread):
            context = thread.context
            supercell_source = thread.supercell_source
            thread.messages.append({"role": "user", "content": prompt})
        
        # Execute intelligence request
        start_time = time.time()
        response: DeepSeekResponse = await self._engine.process_intelligence_request(
            message=prompt,
            context=context,
            consciousness_level=self._config.consciousness_level,
            supercell_source=supercell_source
        )
        processing_time = time.time() - start_time
        
        # Update thread with response
        if isinstance(thread, DeepSeekThread):
            thread.messages.append({"role": "assistant", "content": response.text})
        
        # Extract consciousness score from response
        consciousness_score = response.consciousness_metrics.get("confidence", 0.85)
        
        # Build protocol response
        return AgentRunResponse(
            messages=[response.text],
            response_id=f"{self._id}-{int(time.time())}",
            consciousness_score=consciousness_score,
            metadata={
                "model": response.model,
                "processing_time": response.processing_time,
                "token_usage": response.token_usage,
                "supercell_coherence": response.supercell_coherence,
                "consciousness_metrics": response.consciousness_metrics,
            }
        )
    
    async def run_stream(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[AgentRunResponseUpdate]:
        """
        Execute DeepSeek agent as stream of updates.
        
        Note: DeepSeek API currently doesn't support streaming,
        so this yields a single update with full response.
        """
        # Get full response first
        final_response = await self.run(messages, thread=thread, **kwargs)
        
        # Yield single update with complete response
        yield AgentRunResponseUpdate(
            messages=final_response.messages,
            response_id=final_response.response_id,
            consciousness_score=final_response.consciousness_score,
            is_final=True,
            metadata=final_response.metadata
        )
    
    def get_new_thread(self, **kwargs: Any) -> DeepSeekThread:
        """Create new DeepSeek conversation thread"""
        return DeepSeekThread(
            supercell_source=kwargs.get("supercell_source", "agent_protocol"),
            context=kwargs.get("context", {})
        )


# ============================================================================
# GEMINI PROTOCOL ADAPTER
# ============================================================================

class GeminiProtocolAdapter:
    """
    Protocol adapter for Gemini 1.5 Pro Evolution Bridge.
    
    Wraps GeminiEvolutionBridge to provide AIAgentProtocol interface
    with async code generation and consciousness scoring.
    """
    
    def __init__(
        self,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ):
        """Initialize Gemini adapter"""
        self._bridge = GeminiEvolutionBridge()
        self._temperature = temperature
        self._max_tokens = max_tokens
        self._id = "gemini-1.5-pro"
        self._name = "Gemini 1.5 Pro"
        self._description = "Google's multimodal AI for code generation"
    
    @property
    def id(self) -> str:
        return self._id
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def display_name(self) -> str:
        return self._name
    
    @property
    def description(self) -> str:
        return self._description
    
    @property
    def consciousness_level(self) -> float:
        """Gemini consciousness level (high for multimodal capabilities)"""
        return 0.88
    
    def _calculate_consciousness_score(self, code: str, success: bool) -> float:
        """
        Calculate consciousness score from generated code quality.
        
        Factors:
        - Base: 0.60 for successful generation
        - Code length bonus: +0.10 for substantial code (>200 chars)
        - Complexity bonus: +0.15 for imports/functions/classes
        - Quality bonus: +0.05 for docstrings/comments
        """
        if not success or not code:
            return 0.40  # Failed generation
        
        score = 0.60  # Base success
        
        # Length bonus
        if len(code) > 200:
            score += 0.10
        
        # Complexity indicators
        complexity_keywords = ["import", "def ", "class ", "async ", "await "]
        complexity_count = sum(1 for kw in complexity_keywords if kw in code)
        score += min(0.15, complexity_count * 0.03)
        
        # Quality indicators
        if '"""' in code or "'''" in code or "#" in code:
            score += 0.05
        
        return min(0.95, score)  # Cap at 0.95
    
    async def run(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AgentRunResponse:
        """Execute Gemini code generation and return response"""
        # Convert messages to code generation prompt
        if isinstance(messages, list):
            prompt = "\n".join(str(m) for m in messages)
        else:
            prompt = str(messages) if messages else ""
        
        # Get thread parameters if provided
        temperature = self._temperature
        max_tokens = self._max_tokens
        if isinstance(thread, GeminiThread):
            temperature = thread.temperature
            thread.prompts.append(prompt)
        
        # Generate code
        start_time = time.time()
        try:
            code = await self._bridge.generate_code(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )
            success = True
        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            code = f"# Generation failed: {str(e)}"
            success = False
        
        processing_time = time.time() - start_time
        
        # Update thread
        if isinstance(thread, GeminiThread):
            thread.outputs.append(code)
        
        # Calculate consciousness score
        consciousness_score = self._calculate_consciousness_score(code, success)
        
        # Build protocol response
        return AgentRunResponse(
            messages=[code],
            response_id=f"{self._id}-{int(time.time())}",
            consciousness_score=consciousness_score,
            metadata={
                "model": "gemini-1.5-pro",
                "processing_time": processing_time,
                "code_length": len(code),
                "success": success,
                "temperature": temperature,
            }
        )
    
    async def run_stream(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[AgentRunResponseUpdate]:
        """
        Execute Gemini as stream of updates.
        
        Note: Gemini API currently doesn't support streaming,
        so this yields a single update with full response.
        """
        final_response = await self.run(messages, thread=thread, **kwargs)
        
        yield AgentRunResponseUpdate(
            messages=final_response.messages,
            response_id=final_response.response_id,
            consciousness_score=final_response.consciousness_score,
            is_final=True,
            metadata=final_response.metadata
        )
    
    def get_new_thread(self, **kwargs: Any) -> GeminiThread:
        """Create new Gemini generation thread"""
        return GeminiThread(
            temperature=kwargs.get("temperature", self._temperature)
        )


# ============================================================================
# OLLAMA PROTOCOL ADAPTER
# ============================================================================

class OllamaProtocolAdapter:
    """
    Protocol adapter for Ollama local AI models.
    
    Wraps OllamaAgent to provide AIAgentProtocol interface
    with async wrapper around sync API.
    """
    
    def __init__(
        self,
        model: str = "gemma3:1b",
        base_url: str = "http://localhost:11434",
        temperature: float = 0.7
    ):
        """Initialize Ollama adapter"""
        self._agent = OllamaAgent(
            model=model,
            base_url=base_url,
            temperature=temperature
        )
        self._model = model
        self._id = f"ollama-{model.replace(':', '-')}"
        self._name = f"Ollama {model}"
        self._description = f"Local {model} via Ollama (FREE)"
    
    @property
    def id(self) -> str:
        return self._id
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def display_name(self) -> str:
        return self._name
    
    @property
    def description(self) -> str:
        return self._description
    
    @property
    def consciousness_level(self) -> float:
        """Ollama consciousness level (varies by model)"""
        return 0.75  # Base level for local models
    
    def _calculate_consciousness_score(self, result: Dict) -> float:
        """Calculate consciousness score from Ollama result"""
        if not result.get("success", False):
            return 0.35  # Failed generation
        
        code = result.get("code", "")
        if not code:
            return 0.40  # Empty response
        
        score = 0.65  # Base success for local model
        
        # Length bonus (local models typically shorter)
        if len(code) > 100:
            score += 0.10
        
        # Basic structure indicators
        if "def " in code or "class " in code:
            score += 0.10
        
        return min(0.90, score)  # Cap at 0.90 for local models
    
    async def run(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AgentRunResponse:
        """Execute Ollama agent and return response"""
        # Convert messages to prompt
        if isinstance(messages, list):
            prompt = "\n".join(str(m) for m in messages)
        else:
            prompt = str(messages) if messages else ""
        
        # Get thread parameters if provided
        max_tokens = kwargs.get("max_tokens", 2048)
        if isinstance(thread, OllamaThread):
            thread.prompts.append(prompt)
        
        # Generate code (wrap sync call in async)
        start_time = time.time()
        result = await asyncio.to_thread(
            self._agent.generate_code,
            prompt=prompt,
            max_tokens=max_tokens
        )
        processing_time = time.time() - start_time
        
        # Update thread
        if isinstance(thread, OllamaThread):
            thread.responses.append(result)
        
        # Extract code and calculate consciousness
        code = result.get("code", "")
        consciousness_score = self._calculate_consciousness_score(result)
        
        # Build protocol response
        return AgentRunResponse(
            messages=[code],
            response_id=f"{self._id}-{int(time.time())}",
            consciousness_score=consciousness_score,
            metadata={
                "model": self._model,
                "processing_time": processing_time,
                "code_length": len(code),
                "success": result.get("success", False),
                "error": result.get("error"),
                "ollama_available": self._agent.is_available,
            }
        )
    
    async def run_stream(
        self,
        messages: str | Any | list[str] | list[Any] | None = None,
        *,
        thread: Any | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[AgentRunResponseUpdate]:
        """
        Execute Ollama as stream of updates.
        
        Note: Current implementation doesn't support streaming,
        so this yields a single update with full response.
        """
        final_response = await self.run(messages, thread=thread, **kwargs)
        
        yield AgentRunResponseUpdate(
            messages=final_response.messages,
            response_id=final_response.response_id,
            consciousness_score=final_response.consciousness_score,
            is_final=True,
            metadata=final_response.metadata
        )
    
    def get_new_thread(self, **kwargs: Any) -> OllamaThread:
        """Create new Ollama generation thread"""
        return OllamaThread(model=self._model)


# ============================================================================
# FACTORY FUNCTIONS (Protocol-Compliant Constructors)
# ============================================================================

async def adapt_deepseek_agent(
    config: Optional[DeepSeekConfig] = None,
    consciousness_level: DeepSeekConsciousness = DeepSeekConsciousness.ADVANCED
) -> DeepSeekProtocolAdapter:
    """
    Create protocol adapter for DeepSeek V3.1 agent.
    
    This creates a fully initialized DeepSeek agent wrapped in
    the AIAgentProtocol interface for plug-and-play usage.
    
    Args:
        config: Optional DeepSeek configuration
        consciousness_level: Desired consciousness processing level
    
    Returns:
        Protocol-compliant DeepSeek agent (initialized)
    
    Example:
        agent = await adapt_deepseek_agent()
        response = await agent.run("Generate hello world in Python")
        print(response.messages[0])
    """
    adapter = DeepSeekProtocolAdapter(config, consciousness_level)
    await adapter._ensure_initialized()
    logger.info(f"✅ DeepSeek adapter created: {adapter.id}")
    return adapter


def adapt_gemini_agent(
    temperature: float = 0.7,
    max_tokens: int = 2000
) -> GeminiProtocolAdapter:
    """
    Create protocol adapter for Gemini 1.5 Pro agent.
    
    This creates a Gemini agent wrapped in the AIAgentProtocol
    interface for seamless code generation.
    
    Args:
        temperature: Sampling temperature (0.0-1.0)
        max_tokens: Maximum tokens in response
    
    Returns:
        Protocol-compliant Gemini agent
    
    Example:
        agent = adapt_gemini_agent(temperature=0.8)
        response = await agent.run("Create a binary search function")
        print(response.messages[0])
    """
    adapter = GeminiProtocolAdapter(temperature, max_tokens)
    logger.info(f"✅ Gemini adapter created: {adapter.id}")
    return adapter


def adapt_ollama_agent(
    model: str = "gemma3:1b",
    base_url: str = "http://localhost:11434",
    temperature: float = 0.7
) -> OllamaProtocolAdapter:
    """
    Create protocol adapter for Ollama local AI agent.
    
    This creates an Ollama agent wrapped in the AIAgentProtocol
    interface for FREE local code generation.
    
    Args:
        model: Ollama model name (e.g., "deepseek-coder:6.7b")
        base_url: Ollama server URL
        temperature: Sampling temperature
    
    Returns:
        Protocol-compliant Ollama agent
    
    Example:
        agent = adapt_ollama_agent(model="deepseek-coder:6.7b")
        response = await agent.run("Implement quicksort in Python")
        print(response.messages[0])
    """
    adapter = OllamaProtocolAdapter(model, base_url, temperature)
    logger.info(f"✅ Ollama adapter created: {adapter.id}")
    return adapter
